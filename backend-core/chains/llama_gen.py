import os
import logging
from langchain_huggingface import HuggingFacePipeline
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
import torch

logger = logging.getLogger("Backend-Core-LlamaGen")

# ëª¨ë¸ ë¡œë“œ (HuggingFace Pipeline ì‚¬ìš©)
# ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ëª¨ë¸ ê²½ë¡œë¥¼ í™˜ê²½ ë³€ìˆ˜ë‚˜ ë³¼ë¥¨ ë§ˆìš´íŠ¸ë¡œ ê´€ë¦¬í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ
MODEL_ID = "meta-llama/Llama-3.2-3B-Instruct"

class QuestionGenerator:
    def __init__(self):
        logger.info(f"Loading Llama model with 4-bit quantization: {MODEL_ID}")
        token=os.getenv("HUGGINGFACE_HUB_TOKEN")
        
        # BitsAndBytes 4-bit ì–‘ìí™” ì„¤ì • (VRAM ì‚¬ìš©ëŸ‰: ~4GBë¡œ ì¶•ì†Œ)
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,                    # 4ë¹„íŠ¸ ì–‘ìí™” í™œì„±í™”
            bnb_4bit_compute_dtype=torch.float16, # ì—°ì‚°ì€ FP16ìœ¼ë¡œ ìˆ˜í–‰
            bnb_4bit_use_double_quant=True,       # ì¤‘ì²© ì–‘ìí™” (ë©”ëª¨ë¦¬ ì¶”ê°€ ì ˆê°)
            bnb_4bit_quant_type="nf4"             # NormalFloat4 (LLMì— ìµœì í™”)
        )
        
        logger.info("Initializing tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=token)
        
        logger.info("Loading 4-bit quantized model (this may take 1-2 minutes)...")
        self.model = AutoModelForCausalLM.from_pretrained(
            MODEL_ID,
            quantization_config=quantization_config,
            device_map="auto",                    # GPU ìë™ í• ë‹¹
            dtype=torch.float16,
            low_cpu_mem_usage=True,               # CPU ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì†Œí™”
            token=token
        )
        
        logger.info("âœ… Model loaded successfully with 4-bit quantization")
        logger.info(f"ğŸ“Š Estimated VRAM usage: ~4GB (original: ~16GB)")
        
        # Pipeline ìƒì„±
        pipe = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            max_new_tokens=256,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.1,
            do_sample=True
        )
        self.llm = HuggingFacePipeline(pipeline=pipe)
        
    def generate_questions(self, position: str, count: int = 5):
        prompt = PromptTemplate.from_template(
            """
            ### System:
            ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ ê¸°ìˆ  ë©´ì ‘ê´€ì…ë‹ˆë‹¤. {position} ì§ë¬´ì— ì í•©í•œ ì‹¤ë¬´ ë©´ì ‘ ì§ˆë¬¸ {count}ê°œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
            ì§ˆë¬¸ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ë©°, ê° ì§ˆë¬¸ì€ ì„ ì–¸ì ì¸ ë¬¸ì¥ìœ¼ë¡œ ëª…í™•í•˜ê²Œ í‘œí˜„í•˜ì„¸ìš”.
            ì§ˆë¬¸ ì´ì™¸ì˜ ë¶€ê°€ì ì¸ ì„¤ëª…ì€ ìƒëµí•˜ì‹­ì‹œì˜¤.

            ### Assistant:
            """
        )
        
        chain = prompt | self.llm | StrOutputParser()
        result = chain.invoke({"position": position, "count": count})
        
        # ê°„ë‹¨í•œ íŒŒì‹± (ì¤„ë°”ê¿ˆ ë“±ìœ¼ë¡œ êµ¬ë¶„ëœ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ)
        questions = [q.strip() for q in result.split("\n") if q.strip()]
        return questions[:count]

# ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œ ì´ˆê¸°í™” (API ì‹¤í–‰ ì‹œ ë¡œë“œ)
generator = QuestionGenerator()
